---
title: "Homework 2"
author: "David Toth"
date: "4/29/2021"
output: pdf_document
---
```{r message=FALSE}
library(fda)
library(ISLR)
library(splines)
library(e1071)
library(tree)
library(randomForest)
library(gbm)
```

# Question 1

## Part A
```{r}
days <- 1:365
temp <- as.vector(CanadianWeather$dailyAv[,"Victoria", 2])

fit1 <- lm(temp ~ bs(days, knots = c(90, 181, 273)))
fit2 <- lm(temp ~ bs(days, knots = c(181)))

yhat1 <- predict(fit1, list(day=days), se = TRUE)
yhat2 <- predict(fit2, list(day=days), se = TRUE)

plot(days, temp, xlab = "Day", ylab = "Average Temperature")
lines(days, yhat1$fit, lwd = 2, col="red")
lines(days, yhat1$fit + 2*yhat1$se, lty = "dashed", col = "red")
lines(days, yhat1$fit - 2*yhat1$se, lty = "dashed", col = "red")
lines(days, yhat2$fit, lwd = 2)
lines(days, yhat2$fit + 2*yhat2$se, lty = "dashed")
lines(days, yhat2$fit - 2*yhat2$se, lty = "dashed")
legend("top", legend = c("3 knots", "1 knot"), col = c("red", "black"), lty = 1)
```

## Part B
```{r}
fit3 <- smooth.spline(days, temp, cv = TRUE)

fit3$lambda
fit3$df

plot(days, temp, xlab = "Day", ylab = "Average Temperature")
lines(days, fit3$y, lwd = 2, col = "red")
```
The chosen $\lambda$ is 8.297566e-05. The chosen degrees of freedom is 17.20458.

## Part C
```{r}
fit4 <- loess(temp ~ days, span = 0.2)
fit5 <- loess(temp ~ days, span = 0.5)

plot(days, temp, xlab = "Day", ylab = "Average Temperature")
lines(days, predict(fit4, data.frame(day = days)), col = "red", lwd = 2)
lines(days, predict(fit5, data.frame(day = days)), lwd = 2)
legend("top", legend = c("Span = 0.2", "Span = 0.5"), col = c("red", "black"),
       lty = 1)
```

## Part D
```{r}
mspe <- function(ytrue, yhat){
  mean((ytrue - yhat)^2)
}

test_days <- 274:365
ytrue <- temp[test_days]

yhat11 <- as.vector(yhat1$fit[test_days])
yhat22 <- as.vector(yhat2$fit[test_days])
yhat3 <- fit3$y[test_days]
yhat4 <- as.vector(predict(fit4, data.frame(days = test_days)))
yhat5 <- as.vector(predict(fit5, data.frame(days = test_days)))

mspe_values <- lapply(list(model1 = yhat11, model2 = yhat22, model3 = yhat3, 
                           model4 = yhat4, model5 = yhat5), 
                      mspe, ytrue = ytrue)

data.frame(mspe_values)
```

# Question 2

## Part A
```{r}
svm_linear <- svm(Species ~ Sepal.Width + Sepal.Length, data = iris, kernel = "linear")
svm_rbf <- svm(Species ~ Sepal.Width  + Sepal.Length, data = iris, gamma = 1)

par(mfrow = c(1,2))
plot(svm_linear, iris, Sepal.Width ~ Sepal.Length)
plot(svm_rbf, iris, Sepal.Width ~ Sepal.Length)

table(iris$Species, svm_linear$fitted)
table(iris$Species, svm_rbf$fitted)
```

## Part B
```{r}
set.seed(123)
train <- sample(nrow(iris), 0.5*nrow(iris))
fit <- tune(svm, Species ~ ., data = iris[train,], kernel = "radial", 
            ranges = list(cost = c(0.1, 1, 10, 100),
                          gamma = c(0.5, 1, 2)))

table(iris$Species[-train], predict(fit$best.model, iris[-train,]))
```
The model correctly classifies 27 + 25 + 19 = 71 cases in the test set. This corresponds to a 94.67% classification rate. 

## Part C
```{r}
rbf_acc <- rep(0, 10)
linear_acc <- rep(0, 10)

for(i in 1:10){
  train <- sample(nrow(iris), 0.5*nrow(iris))
  fit_rbf <- tune(svm, Species ~ ., data = iris[train,], 
                  ranges = list(cost = c(0.1, 1, 10, 100),
                                gamma = c(0.5, 1, 2)))
  fit_linear <- tune(svm, Species ~ ., data = iris[train,], kernel = "linear",
                     ranges = list(cost = c(0.1, 1, 10, 100)))
  
  yhat_rbf <- predict(fit_rbf$best.model, iris[-train,])
  yhat_linear <- predict(fit_linear$best.model, iris[-train,])
  
  rbf_acc[i] <- sum(yhat_rbf == iris$Species[-train])/length(yhat_rbf)
  linear_acc[i] <- sum(yhat_linear == iris$Species[-train])/length(yhat_linear)
}

mean(rbf_acc)
mean(linear_acc)
```
On average, the linear kernel performs better by 1.2 percentage points.

# Question 3

## Part A
```{r}
set.seed(123)
train <- sample(nrow(Carseats), 0.5*nrow(Carseats))

tree1 <- tree(Sales ~ CompPrice + Income + Advertising + Population + 
                   Price + Age + Education, data = Carseats, subset = train)

tree_cv <- cv.tree(tree1)

plot(tree_cv$size, tree_cv$dev, type = "b")

tree_pruned <- prune.tree(tree1, best = 12)

plot(tree_pruned)
text(tree_pruned, pretty = 0, cex = 0.5)
```
Starting from the top, if the price is less than 105.5, we proceed to the left. Otherwise, we proceed to the right. At each node, if the condition is met, we proceed down to the left; if it is not met, we proceed down to the right. This process continues until a terminal node is reached. The terminal nodes are the tree's predicted value. 

## Part B
```{r}
forest1 <- randomForest(Sales ~ CompPrice + Income + Advertising + Population + 
                   Price + Age + Education, data = Carseats, subset = train, 
                   mtry = 7, importance = TRUE)

forest2 <- randomForest(Sales ~ CompPrice + Income + Advertising + Population + 
                   Price + Age + Education, data = Carseats, subset = train, 
                   mtry = 3, importance = TRUE)

yhat1 <- predict(forest1, Carseats[-train,])
yhat2 <- predict(forest2, Carseats[-train,])
ytrue <- Carseats$Sales[-train]

plot(ytrue, yhat1, xlab = "True Value", ylab = "Predicted Value")
title("mtry = 7")
plot(ytrue, yhat2, xlab = "True Value", ylab = "Predicted Value")
title("mtry = 3")

mse_1 <- mean((yhat1 - ytrue)^2)
mse_2 <- mean((yhat2 - ytrue)^2)

data.frame(mse_model1 = mse_1,
           mse_model2 = mse_2)

forest1$importance
forest2$importance
```
The first model, the one with `mtry = 7`, has a slightly lower MSE. 

Using the percent decrease in MSE as the measure for variable importance, `forest1` ranks the variables as follows: Price > CompPrice > Age > Advertising > Population > Education > Income. Likewise, for `forest2`, the rankings are: Price > Age > CompPrice > Advertising > Population > Income > Education. You can see that some of the variables are ranked differently in the two models. 

## Part C
```{r}
set.seed(123)
boosted_fit <- gbm(Sales ~ CompPrice + Income + Advertising + Population + 
                   Price + Age + Education, data = Carseats[train,], 
                   distribution = "gaussian", n.trees = 5000)

summary(boosted_fit)

mse_test <- mean((predict(boosted_fit, Carseats[-train,]) - Carseats$Sales[-train])^2)

mse_test
```
Price is the most important predictor and accounts for 17.15% of the reduction in the loss for this set of test data. The interpretations for the other variables follow the same pattern: [Variable] accounts for X% of the reduction in the loss for this set of test data. 

The MSE is larger than the bagging/random forests in part b. 